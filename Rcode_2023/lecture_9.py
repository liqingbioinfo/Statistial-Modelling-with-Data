# -*- coding: utf-8 -*-
"""Lecture_9.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1gxu7EHVYtp-DZho2suL_1dD3a3huDinw
"""

# TensorFlow and tf.keras
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras.layers import Flatten, Dense, Conv2D, MaxPooling2D, Dropout

# Commonly used modules
import numpy as np
import pandas as pd
import os
import sys

print(tf.__version__)

# Plot packages
import matplotlib.pyplot as plt

"""# Exercise 4: Build a simple NN model to classify figures"""

### Build CNN model for images
model = keras.Sequential()
# 32 convolution filters used each of size 3x3
model.add(Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=(28, 28, 1)))
# 64 convolution filters used each of size 3x3
model.add(Conv2D(64, (3, 3), activation='relu'))
# choose the best features via pooling
model.add(MaxPooling2D(pool_size=(2, 2)))
# flatten since too many dimensions, we only want a classification output
model.add(Flatten())
# fully connected to get all relevant data
model.add(Dense(128, activation='relu'))
# output a softmax to squash the matrix into output probabilities
model.add(Dense(10, activation='softmax'))


model.compile(optimizer=tf.optimizers.Adam(), 
              loss='sparse_categorical_crossentropy',
              metrics=['accuracy'])

### Load data
(train_images, train_labels), (test_images, test_labels) = keras.datasets.mnist.load_data()

# reshape images to specify that it's a single channel
train_images = train_images.reshape(train_images.shape[0], 28, 28, 1)
test_images = test_images.reshape(test_images.shape[0], 28, 28, 1)

def preprocess_images(imgs): # should work for both a single image and multiple images
    sample_img = imgs if len(imgs.shape) == 2 else imgs[0]
    assert sample_img.shape in [(28, 28, 1), (28, 28)], sample_img.shape # make sure images are 28x28 and single-channel (grayscale)
    return imgs / 255.0

train_images = preprocess_images(train_images)
test_images = preprocess_images(test_images)

### Train models
history = model.fit(train_images, train_labels, epochs=5)

hist = pd.DataFrame(history.history)
hist.head(3)

# Show cross entropy loss
cross_entropy = float(hist['accuracy'].tail(1))
print()
print('Final cross entropy on training set: {}'.format(round(cross_entropy, 3)))

### Evaluate model's accuracy in test dataset
print(test_images.shape)
test_loss, test_acc = model.evaluate(test_images, test_labels)
print('Test accuracy:', test_acc)

"""# Exercise 5: Transfer learning

### 5.1. Load and examine pre-trained MobileNetV2
"""

IMG_SIZE = (160, 160) # minimum image shape is (32, 32)

# Create the base model from the pre-trained model MobileNet V2
IMG_SHAPE = IMG_SIZE + (3,)
base_model = tf.keras.applications.MobileNetV2(input_shape=IMG_SHAPE, #default is (224,224,3)
                                               include_top=False,
                                               weights='imagenet')

base_model.trainable = False  # Freeze the base model

# Let's take a look at the base model architecture
# base_model.summary()

print("Number of layers in the base model: ", len(base_model.layers))

print("Shape of last layer: ", base_model.layers[-1].output_shape)

"""### 5.2. Creat a new model on top of the base model"""

#Add three layers
global_average_layer = tf.keras.layers.GlobalAveragePooling2D()
dropout_layer = tf.keras.layers.Dropout(0.2)
prediction_layer = tf.keras.layers.Dense(1)

# Image data augmentation
data_augmentation = tf.keras.Sequential([
  tf.keras.layers.RandomFlip('horizontal'),
  tf.keras.layers.RandomRotation(0.2),
])

# Rescale pixel values from [0, 255] -> [-1.0, 1.0]
rescale = tf.keras.layers.Rescaling(1./127.5, offset=-1)

#New model
inputs = tf.keras.Input(shape=IMG_SHAPE)
x = data_augmentation(inputs)
x = rescale(x) 
x = base_model(x, training=False)
x = global_average_layer(x)
x = dropout_layer(x)
outputs = prediction_layer(x)
model = tf.keras.Model(inputs, outputs)

#Compile it
base_learning_rate = 0.0001
model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=base_learning_rate),
              loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),
              metrics=['accuracy'])

"""### 5.3. Load data"""

# Run the two lines of code below on a login node because our compute nodes don't have internet access.
# It caches the downloaded data in ~/.keras
_URL = 'https://storage.googleapis.com/mledu-datasets/cats_and_dogs_filtered.zip'
path_to_zip = tf.keras.utils.get_file('cats_and_dogs.zip', origin=_URL, extract=True)
PATH = os.path.join(os.path.dirname(path_to_zip), 'cats_and_dogs_filtered')

train_dir = os.path.join(PATH, 'train')
validation_dir = os.path.join(PATH, 'validation')

BATCH_SIZE = 32

train_dataset = tf.keras.utils.image_dataset_from_directory(train_dir,
                                                            shuffle=True,
                                                            batch_size=BATCH_SIZE,
                                                            image_size=IMG_SIZE)

validation_dataset = tf.keras.utils.image_dataset_from_directory(validation_dir,
                                                                 shuffle=True,
                                                                 batch_size=BATCH_SIZE,
                                                                 image_size=IMG_SIZE)

# Show the first nine images and labels from the training set:
class_names = train_dataset.class_names

plt.figure(figsize=(10, 10))
for images, labels in train_dataset.take(1):
  for i in range(9):
    ax = plt.subplot(3, 3, i + 1)
    plt.imshow(images[i].numpy().astype("uint8"))
    plt.title(class_names[labels[i]])
    plt.axis("off")

# Regard valication_dataset as test_dataset
val_batches = tf.data.experimental.cardinality(validation_dataset)
test_dataset = validation_dataset

print('Number of training batches: %d' % tf.data.experimental.cardinality(train_dataset))
print('Number of test batches: %d' % tf.data.experimental.cardinality(test_dataset))

# Enable Auto-prefetching to prevent image data I/O blocking
AUTOTUNE = tf.data.AUTOTUNE
train_dataset = train_dataset.prefetch(buffer_size=AUTOTUNE)
test_dataset = test_dataset.prefetch(buffer_size=AUTOTUNE)

"""### 5.4. Train and evaluate the new model"""

initial_epochs = 20
history = model.fit(train_dataset,
                    epochs=initial_epochs)

### Evaluate model's accuracy in test dataset
print(test_images.shape)
loss, accuracy = model.evaluate(test_dataset)
print('Test accuracy :', accuracy)