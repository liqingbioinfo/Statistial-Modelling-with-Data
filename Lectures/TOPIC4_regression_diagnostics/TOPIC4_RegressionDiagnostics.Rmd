---
title: "TOPIC 4: Regression Model Diagnostics"
output:
  pdf_document: default
  html_notebook: default
  html_document:
    df_print: paged
  word_document: default
---

```{=html}
<style type="text/css">

body{ /* Normal  */
      font-size: 18px;
  }
td {  /* Table  */
  font-size: 8px;
}
.title {
  font-size: 38px;
  color: DarkRed;
}
 p {line-height: 2em;}
h1 { /* Header 1 */
  font-size: 28px;
  color: DarkBlue;
}
h2 { /* Header 2 */
    font-size: 22px;
  color: DarkBlue;
}
h3 { /* Header 3 */
  font-size: 18px;
  font-family: "Times New Roman", Times, serif;
  color: DarkBlue;
}
code.r{ /* Code block */
    font-size: 12px;
}
pre { /* Code block - determines code spacing between lines */
    font-size: 14px;
}
</style>
```
```{r message=FALSE, warning=FALSE, include=FALSE}
knitr::opts_knit$set(root.dir = 'c:/users/pgalpern/onedrive - university of calgary/teaching/data603/data/datasets_pictures')

```

## Multiple Linear Regression 
## Part IV: MULTIPLE REGRESSION DIAGNOSTICS

&copy; Thuntida Ngamkham 2022 (edited by Paul Galpern)


## Residual Analysis: Checking the Regression Assumptions

Most statistical tests rely upon certain assumptions about the variables used in the analysis. __When these assumptions are not met the results may not be trustworthy__. The assumptions and conditions for the multiple regression model sound nearly the same as for simple linear regression, but with more variables in the model.

## 1. Linearity Assumption

The linear regression model assumes that there is a straight-line relationship between the predictors and the response. If the true relationship is far from linear, then virtually all of the conclusions that we draw from the fit are suspect. In addition, the prediction accuracy of the model can be significantly reduced.

**Residual plots** are a useful graphical tool for identifying non-linearity. In the case of multiple regression model since there are multiple predictors, we instead plot the residuals versus predicted (or fitted) values $\hat{y_i}$. Ideally, the residual plot will show no discernible pattern. The presence of a presence may indicate a problem with some aspect of the linear model.

If the residual plot indicates that there are non-linear associations in the data, then a simple approach is to use non-linear transformations of the predictors, such as $\log(X)$, $\sqrt{X}$, and $X^2$, in the regression model. 

![This scatter plot shows the distribution of residuals (errors) vs fitted values (predicted values)](c:/users/pgalpern/onedrive - university of calgary/teaching/data603/data/datasets_pictures/linearity assumption.png)


```{r}
library(ggplot2)
Advertising=read.table("Advertising.txt", header = TRUE,  sep ="\t" )
model<-lm(sale~tv+radio+tv*radio, data=Advertising)
summary(model)
ggplot(model, aes(x=.fitted, y=.resid)) +
  geom_point() + geom_smooth()+
  geom_hline(yintercept = 0) 

#optional graph from plot()
plot(fitted(model), residuals(model),xlab="Fitted Values", ylab="Residuals")
abline(h=0,lty=1)
title("Residual vs Fitted")
```

*R functions*

*ggplot(model, aes(x=...., y=...)): mapping 2 variables on a and y axis*

*.fitted        : Fitted values of model*

*.resid         : Residuals*

*geom_hline(yintercept = 0):add a horizontal line*

*geom_point()   : add a layer of points to the plot*

From the Advertising example, the output displays the residual plot that results from the model$\hat{Sale}=  6.750+0.01910tv+0.02886radio+0.001086tv*radio$. There appears to be a little pattern in the residuals, suggesting that the quadratic term or logarithmic might improve the fit to the data.

<!-- Change on the code -->
```{r}
library(ggplot2)
Advertising=read.table("Advertising.txt", header = TRUE,sep ="\t" )
library(GGally)
ggpairs(Advertising,lower = list(continuous = "smooth_loess", combo =
  "facethist", discrete = "facetbar", na = "na"))
model<-lm(sale~tv+radio+tv*radio, data=Advertising)
quadmodel<-lm(sale~tv+I(tv^2)+radio+tv*radio, data=Advertising)
cubic<-lm(sale~tv+I(tv^2)+I(tv^3)+radio+tv*radio, data=Advertising)
### OR
# cubic<-lm(sale~poly(tv,3,raw=T)+radio+tv*radio, data=Advertising)

summary(model)$adj.r.squared
summary(quadmodel)$adj.r.squared
summary(cubic)$adj.r.squared
summary(cubic)

#residual vs fitted data plot for the simple  model
ggplot(model, aes(x=.fitted, y=.resid)) +
  geom_point() +geom_smooth()+
  geom_hline(yintercept = 0) 
#residual vs fitted data plot for the quadratic  model
ggplot(quadmodel, aes(x=.fitted, y=.resid)) +
  geom_point() +geom_smooth()+
  geom_hline(yintercept = 0) 


#residual vs fitted data plot for the cubic model
ggplot(cubic, aes(x=.fitted, y=.resid)) +
  geom_point() +geom_smooth()+
  geom_hline(yintercept = 0) 

```


From the output, there appears to be a little pattern for the  quadratic regresssion model while the cubic model shows no pattern of the residuals at all. Moreover, the $R^2-{adj}$ of the cubic model is 0.9907 indicates the variation in $y$ that can be explained by this model is 99.02% with RMSE= 0.5026. Therefore, we can conclude that the cubic model is the best fit model to predict $Y$ among the models we considered.

------------------------------------------------------------------

## Inclass Practice Problem 16

From the clerical staff work hours, use residual plots to conduct a residual analysis of the data.  Check Linearity Assumption.  If a trend is detected, how would you like to transform the predictors in the model?

```{r, include=F}
workhours=read.csv("CLERICAL.csv",
                header = TRUE) 
model1<-lm(Y~X2+X4+X5,data=workhours) 
summary(model1)
library(ggplot2)
ggplot(model1, aes(x=.fitted, y=.resid)) +
  geom_point() +geom_smooth()+
  geom_hline(yintercept = 0) 
#plot(model1)
library(GGally)
ggpairs(data.frame(Y=workhours$Y,X2=workhours$X2,X4=workhours$X4,X5=workhours$X5),lower = list(continuous = "smooth_loess", combo ="facethist", discrete = "facetbar", na = "na"))

#residual vs fitted data plot for the second order model


model<-lm(Y~X2+I(X2^2)+X4+X5,data=workhours)
summary(model)
#plot(model)
#residual vs fitted data plot for the second order model
ggplot(model, aes(x=.fitted, y=.resid)) +
  geom_point() +geom_smooth()+
  geom_hline(yintercept = 0) 

```

## 2. Independence Assumption

An important assumption of the linear regression model is that the error terms, $\epsilon_1,\epsilon_2,\epsilon_3,...,\epsilon_n$ are uncorrelated (must be mutually independent). What does this mean? For instance, if the errors are uncorrelated, then the fact that $\epsilon_i$ is positive provides little or no information about the sign of $\epsilon_{i+1}$. 

The assumption of independent errors is violated when successive errors are correlated. This typically occurs when the data for both dependent and independent variables are observed sequentially over a period of time-called __time-series data__

We can check displays of the regression residuals for evidence of patterns, trends or clumping, any of which would suggest a failure of independence. In the special case when response $Y$ is related to time (time series data), a common violation of the Independence Assumption is for the errors to be correlated. This violation can be check by plotting the residuals against the order of occurrence (time plot of the residuals and looking for pattern).

In the Advertising example, the subjects were not related to time, so we can pretty sure that their measurement are independent. 

-------------------------------------------------------------------------

## 3. Equal Variance Assumption

Another important assumption of the linear regression model is that the error terms have a constant variance (homoscedasticity), $Var(\epsilon_i) = \sigma^2$. Unfortunately, it is often the case that the variances of the error terms are non-constant. For instance, the variances of the error terms may increase with the value of the response. One can identify non-constant variances in the errors, or **heteroscedasticity**


Heteroscedasticity means unequal scatter. In regression analysis, heteroscedasticity is a systematic change in the spread of the residuals over the range of measured values.  An example is shown in the left-hand panel of the figure below, in which the magnitude of the residuals tends to increase with the fitted values. When faced with this problem, one possible solution is to transform the response $Y$ using a concave function such as $\log(Y)$ or $\sqrt{X}$. Such a transformation results in a greater amount of shrinkage of the larger responses, leading to a reduction in heteroscedasticity. The right-hand panel of the figure below displays the residual plot after transforming the response.

![Residual plots. ](c:/users/pgalpern/onedrive - university of calgary/teaching/data603/data/datasets_pictures/heteroscedasticity.png)


*In each plot, the red line is a smooth fit to the residuals, intended to make it easier to identify a trend. The blue lines track the outer quantiles of the residuals, and emphasize patterns.*

*Left: The funnel shape indicates heteroscedasticity. *

*Right: The predictor has been log-transformed, and there is now no evidence of heteroscedasticity.*

![Residual plots. ](c:/users/pgalpern/onedrive - university of calgary/teaching/data603/data/datasets_pictures/scalelocation.png)


A scale-location plot between fitted value and standardized residuals can also be checked for heteroscedasticity. It's also called Spread-Location plot. This plot shows if residuals are spread equally along the ranges of predictors. You can check the assumption of equal variance (homoscedasticity). It's good if you see a horizontal line with equally (randomly) spread points. From the figure above in Case 1, the residuals appear randomly spread. Whereas, in Case 2, the residuals begin to spread wider along the x-axis as it passes around 5. Because the residuals spread wider and wider, the red smooth line is not horizontal and shows a steep angle in Case 2.

```{r}
library(ggplot2)
Advertising=read.table("Advertising.txt", header = TRUE,sep ="\t" )
cubic<-lm(sale~tv+I(tv^2)+I(tv^3)+radio+tv*radio, data=Advertising)
summary(cubic)
#residuals plot
ggplot(cubic, aes(x=.fitted, y=.resid)) +
  geom_point() +
  geom_hline(yintercept = 0) +
  geom_smooth()+
  ggtitle("Residual plot: Residual vs Fitted values")  

#a scale location plot
ggplot(cubic, aes(x=.fitted, y=sqrt(abs(.stdresid)))) +
  geom_point() +
  geom_hline(yintercept = 0) +
  geom_smooth()+
   ggtitle("Scale-Location plot : Standardized Residual vs Fitted values") 

#optional graphs for residual plots and a scale location plot
plot(cubic, which=1) #residuals plot
plot(cubic, which=3) #a scale location plot
```

*R functions*
*ggplot(model, aes(x=...., y=...)): mapping 2 variables on a and y axis*
*.fitted        : Fitted values of model*
*.resid         : Residuals*
*.stdresid      : Standardised Residuals*
*geom_hline(yintercept = 0):add a horizontal line*
*geom_point()   : add a layer of points to the plot*
*ggtitle()      ; add a title to the plot*


__From the Advertising example__, the output displays the residual plot and Scale-Location plot that result from the cubic model. In our case,  the residuals tend to form a horizontal band-indicates that the plot does not provide evidence to suggest that heteroscedasticity exists.


A more formal, mathematical way of detecting heteroscedasticity is what is known as __the Breusch-Pagan test__. It involves using a variance function and using a $\chi^2 test$ to test 

$$
\begin{aligned}
H_0:&\mbox{ heteroscedasticity is not present (homoscedasticity)}\\
H_a~:&\mbox{ heteroscedasticity is present} \\
or \\
H_0:& \sigma^2_1=\sigma^2_2=...=\sigma^2_n\\
H_0:& \mbox{ at least }\sigma^2_i\mbox{ is different from the others } i=1,2,...,n\\\\
\chi^2&=nR^2 \sim\chi^2_{p-1}\\
where\\
n&=\mbox{sample size}\\
R^2&=\mbox{coefficient determination}\\
p&=\mbox{ number of regression coefficients}
\end{aligned}
$$


```{r}
library(lmtest)
Advertising=read.table("Advertising.txt", header = TRUE,  sep ="\t" )
cubic<-lm(sale~tv+I(tv^2)+I(tv^3)+radio+tv*radio, data=Advertising)
bptest(cubic)
morepower<-lm(sale~tv+I(tv^2)+I(tv^3)+I(tv^4)+I(tv^5)+I(tv^6)+I(tv^7)+
                I(tv^8)+I(tv^9)+I(tv^10)+I(tv^11)+radio+tv*radio, 
              data=Advertising)
summary(morepower)
bptest(morepower)
morepower1<-lm(sale~poly(tv,11,raw = T)+radio+tv*radio, data=Advertising)
summary(morepower1)

```
*R function*
*bptest(): to perform the Breusch-Pagan test*

__From the Advertising example__, the output displays the Breusch-Pagan test that results from the cubic model.  The p-value = 0.00034 <0.05, indicating that we do  reject the null hypothesis. Therefore, the test  provides evidence to suggest that heteroscedasticity does exist. However, a model with more power on tv (power of 11) shows evidence to suggest that heteroscedasticity does not exist.


## Inclass Practice Problem 17

From the clerical staff work hours, use residual plots to conduct a residual analysis of the data.  Check Equal Variance Assumption by graphs and the Breusch-Pagan test.  If you detect a trend, how would you like to transform the predictors in the model?

```{r include=F}
library(ggplot2)
library(lmtest)
workhours=read.csv("CLERICAL.csv",
                header = TRUE)
improvemodel<-lm(Y~X2+I(X2^2)+X4+X5,data=workhours)
bptest(improvemodel)
#residuals plot
ggplot(improvemodel, aes(x=.fitted, y=.resid)) +
  geom_point() +
  geom_hline(yintercept = 0) +
  geom_smooth()+
  ggtitle("Residual plot: Residual vs Fitted values")  

#a scale location plot
ggplot(improvemodel, aes(x=.fitted, y=sqrt(abs(.stdresid)))) +
  geom_point() +
  geom_hline(yintercept = 0) +
  geom_smooth()+
   ggtitle("Scale-Location plot : Standardized Residual vs Fitted values") 

#optional graphs for residual plots and a scale location plot
plot(improvemodel, which=1) #residuals plot
plot(improvemodel, which=3) #a scale location plot
```

*R function*

*geom_smooth()  : add the regression slope*


## 4. Normality Assumption

The multiple linear regression analysis requires that the errors between observed and predicted values (i.e., the residuals of the regression) should be normally distributed. This assumption may be checked by looking at a histogram, a normal probability plot or a Q-Q-Plot.  

If the distribution is normal, the points on such a plot (Probability Plot or Q-Q-Plot) should fall close to the diagonal reference line. A bow-shaped pattern of deviations from the diagonal indicates that the residuals have excessive skewness. An S-shaped pattern of deviations indicates that the residuals have excessive kurtosis, i.e., there are either too many or two few large errors in both directions.  Sometimes the problem is revealed to be that there are a few data points on one or both ends that deviate significantly from the reference line ("outliers"), in which case they should get close attention. 


There are also a variety of statistical tests for normality, including the Kolmogorov-Smirnov test and the Shapiro-Wilk test.

$$
\begin{aligned}
H_0:&\mbox{ the sample data are significantly normally distributed}\\
H_a:&\mbox{ the sample data are not significantly normally distributed } \\
\end{aligned}
$$
```{r}
library(ggplot2)
Advertising=read.table("Advertising.txt", header = TRUE,  sep ="\t" )
#logmodel<-lm(sale~log(tv)+radio+tv*radio, data=Advertising)
morepower<-lm(sale~poly(tv,11,raw = T)+radio+tv*radio, data=Advertising)
#option 1 (histogram)
qplot(residuals(morepower),
      geom="histogram",
      binwidth = 0.1,  
      main = "Histogram of residuals", 
      xlab = "residuals", color="red", 
      fill=I("blue"))
#option 2 (histogram)
ggplot(data=Advertising, aes(residuals(morepower))) + 
  geom_histogram(breaks = seq(-1,1,by=0.1), col="red", fill="blue") + 
  labs(title="Histogram for residuals") +
  labs(x="residuals", y="Count")

#normal QQ plot
ggplot(Advertising, aes(sample=morepower$residuals)) +
  stat_qq() +
  stat_qq_line()

#optional histogram
par(mfrow=c(1,2))
hist(residuals(morepower))
plot(morepower, which=2) #a Normal plot

#Testing for Normality
shapiro.test(residuals(morepower))
```

*R funtions*
*stat_qq() : produce quantile-quantile plots*
*stat_qq_line() : compute the slope and intercept of the line connecting the points at specified quartiles of the theoretical and sample distributions*

geom_qq_line and stat_qq_line compute the slope and intercept of the line connecting the points at specified quartiles of the theoretical and sample distributions.
The outputs show that the residual data have normal distribution (from histogram and Q-Q plot). Moreover, Shapiro-Wilk normality test also confirms that the residuals are normally distributed as the p-value=0.3129 >0.05.


## Inclass Practice Problem 18

From the clerical staff work hours, use residual plots to conduct a residual analysis of the data.  __Check Normality Assumption by graphs and the Shapiro-Wilk normality test__.  If you detect a trend, how would you like to transform the predictors in the model?

```{r eval=FALSE, include=F}
workhours=read.csv("CLERICAL.csv",
                header = TRUE) 
improvemodel<-lm(Y~X2+I(X2^2)+X4+X5,data=workhours)
#option 1 (histogram)
qplot(residuals(improvemodel),
      geom="histogram",
      binwidth = 10,  
      main = "Histogram of residuals", 
      xlab = "residuals", color="red", 
      fill=I("blue"))
#option 2 (histogram)
ggplot(data=workhours, aes(residuals(improvemodel))) + 
  geom_histogram(breaks = seq(-30,30,by=10), col="red", fill="blue") + 
  labs(title="Histogram for residuals") +
  labs(x="residuals", y="Count")

#normal QQ plot
ggplot(workhours, aes(sample=improvemodel$residuals)) +
  stat_qq() +
  stat_qq_line()

#optional histogram
par(mfrow=c(1,2))
hist(residuals(improvemodel))
plot(improvemodel, which=2) #a Normal plot

#Testing for Normality
shapiro.test(residuals(improvemodel))
```


## 5. Multicollinearity

Often, two or more of the independent variables used in the model for $E(Y)$ provide redundant information. That is, the independent variables will be correlated with each other. For example, suppose we want to construct a model to predict the gasoline mileage rating, $Y$, of a truck as a function of its load, $X_1$, and the horsepower, $X_2$, of its engine. In general, you would expect heavier loads to require greater horsepower and to result in lower mileage ratings. Thus, although both $X_1$ and $X_2$ contribute information for the prediction of mileage rating, some of the information is overlapping, because $X_1$ and $X_2$ are (linearly) correlated. When the independent variables are (linearly) correlated, we say that multicollinearity exists. In practice, it is not uncommon to observe correlations among the independent variables. However, a few problems arise when serious multicollinearity is present in the regression analysis.

![The scatter plot shows multicollonearity between Rating and Limit](c:/users/pgalpern/onedrive - university of calgary/teaching/data603/data/datasets_pictures/multicollinearity.png)




In the left-hand panel of Figure 3, the two predictors limit and age appear to have no obvious relationship. In contrast, in the right-hand panel of Figure 3, the predictors limit and rating are very highly linearly correlated with each other, and we say that they are collinear.

### What Problems Do Multicollinearity Cause?

Multicollinearity causes the following two basic types of problems:

1. The coefficient estimates can swing wildly based on which other independent variables are in the model. The coefficients become very sensitive to small changes in the model.

2. Multicollinearity reduces the precision of the estimate coefficients, which weakens the statistical power of your regression model. You might not be able to trust the p-values to identify independent variables that are statistically significant.


### Testing for Multicollinearity with Variance Inflation Factors (VIF)

If you can identify which variables are affected by multicollinearity and the strength of the correlation, you're well on your way to determining whether you need to fix it. Fortunately, there is a very simple test to assess multicollinearity in your regression model which is called __"The variance inflation factor (VIF)"__

The variance inflation factor (VIF) 

VIF identifies correlation between independent variables and the strength of that correlation. It can be computed using the formula
$$
\begin{aligned}
VIF({\hat{\beta_j}})=\frac{1}{1-R^2_{X_j|X_j}}
\end{aligned}
$$
where $R^2_{X_j|X_-j}$ is the $R^2$ from a regression of $X_j$ onto all of the other predictors. If $R^2_{X_j|X_-j}$ is close to one, then collinearity is present, and so the VIF will be large. 



Statistical software calculates a VIF for each independent variable. Value of VIFs start at 1 and have no upper limit and can be interpreted as following;

*VIFs=1 indicates that there is no collinearity between this independent variable and any others. 

*1<= VIFs <=5  suggest that there is a moderate collinearity, but it is not severe enough to warrant corrective measures. 

VIFs > 5 or 10 represent critical levels of multicollinearity where the coefficients are poorly estimated, and the p-values are questionable.

We use VIFs to identify correlations between variables and determine the strength of the relationships. 

**Attention**: *When the high VIFs are caused by the inclusion of powers or products of other variables, you can Safely Ignore Multicollinearity*
```{r,include=F}
X1=rnorm(100);X2=X1^2;mX=mean(X1);X1m=X1-mX;X2m=(X1-mX)^2;Y=2*X1+X2+rnorm(100);
cor(X1,X2);cor(X1m,X2m)
summary(lm(Y~X1+X2));
summary(lm(Y~X1m+X2m))

```


From Advertising data, check Multicollinearity Assumption by using scatter plots and VIF

```{r}
library(mctest) #for VIF
Advertising=read.table("Advertising.txt", header = TRUE,  sep ="\t" )
#logmodel<-lm(sale~log(tv)+radio+tv*radio, data=Advertising)
firstordermodel<-lm(sale~tv+radio, data=Advertising)
pairs(~tv+radio, data=Advertising)
#Calculate VIF for multicollinearity model
#option 1 

imcdiag(firstordermodel, method="VIF")

#option 2
library(car)
vif(firstordermodel)
```
From the output, you can see that the $VIF_{TV}=VIF_{Radio}$=1.003013, which suggests that there is no correlation between these predictors.


From the clerical staff work hours, checking for Multicollinearity by scatter plots between independent predictors and VIF test.  Note! consider only main effect perdictors
```{r}
library(mctest) #for VIF
workhours=read.csv("CLERICAL.csv",header = TRUE) 
#improvemodel<-lm(Y~X2+I(X2^2)+X4+X5,data=workhours)

firstordermodel<-lm(Y~X2+X4+X5,data=workhours)
pairs(~Y+X2+X4+X5,data=workhours)
#Calculate VIF for multicollinearity model
#option 1 

imcdiag(firstordermodel, method="VIF")

#option 2
library(car)
vif(firstordermodel)
#vif(improvemodel)
```


*R functions*
*imcdiag(x=,y=,method="VIF") :  detects the existence of multicollinearity due to independent variables*


## Inclass practice Problem 19

From the credit card example, check for Multicollinearity  by scatter plots between independent predictors and VIF test .  Note! consider only main effect predictors

```{r message=FALSE, warning=FALSE, include=F}
library(mctest) #for VIF
library(GGally)
credit <- read.csv("credit.csv")
multimodel<-lm(Balance~Income+Rating+Age+Limit+Cards+factor(Student),data=credit)
summary(multimodel)

#Calculate VIF for multicollinearity model
X <- data.frame(Age=credit$Age, Rating=credit$Rating, Cards=credit$Cards, Student=credit$Student, Income=credit$Income)
ggpairs(X)
#option 1 
imcdiag(multimodel, method="VIF")
#option 2
library(car)
vif(multimodel)

```


In the credit data example, a regression of balance on Age, Rating, and Limit indicates that the predictors have VIF values of 2.776906 , 230.869514, 1.039696, 229.238479, 1.439007, and 1.009064. As we suspected, there is considerable multicollinearity in the data! When faced with the problem of multicollinearity.

__There are two simple solutions.__

__The first solution__ is to drop one of the problematic variables from the regression model. This can usually be done without much compromise to the regression model, since the presence of multicollinearity implies that the information that this variable provides about the response is redundant in the presence of the other variables. 

__The second solution__ is to combine the collinear variables together into a single predictor. For instance, we might take the average of standardized versions of Limit and Rating in order to create a new variable that measures credit worthiness.



## Inclass Practice Problem 20

From the credit card example, after dropping the Limit variable, find the best model and check for Multicollinearity by scatter plots between independent predictors and VIF test.  Note! consider only main effect perdictors

```{r, include=F}
library(GGally)
library(mctest) #for VIF
credit=read.csv("credit.csv", header = TRUE)
nomultimodel<-lm(Balance~Income+Rating+Age+Cards+factor(Student),data=credit)
summary(nomultimodel)

#Calculate VIF for multicollinearity model
#option 1 
X<-data.frame(Age=credit$Age, Rating=credit$Rating, Cards=credit$Cards, Student= credit$Student,
         Income=credit$Income)
ggpairs(X)
imcdiag(nomultimodel, method="VIF")
#option 2

library(car)
vif(nomultimodel)
```


## 6. Outlier (The Effect on Individual Cases)

An outlying case is defined as a particular observation ($Y,X_1,X_2,...X_p$) that differs from the majority of the cases in the data set. There are several ways we can find and evaluate outlier or influential points.


__1. Residuals vs Leverage plot__

This plot helps us to find influential cases (i.e., subjects) if any. Not all outliers are influential in linear regression analysis (whatever outliers mean). Even though data have extreme values, they might not be influential to determine a regression line. That means, the results wouldn't be much different if we either include or exclude them from analysis. They follow the trend in the majority of cases and they don't really matter; they are not influential. On the other hand, some cases could be very influential even if they look to be within a reasonable range of the values. They could be extreme cases against a regression line and can alter the results if we exclude them from analysis. Another way to put it is that they don't get along with the trend in the majority of the cases.

Unlike the other plots, this time patterns are not relevant. We watch out for outlying values at the upper right corner or at the lower right corner. Those spots are the places where cases can be influential against a regression line. Look for cases outside of a dashed line, Cook's distance. When cases are outside of the Cook's distance (meaning they have high Cook's distance scores), the cases are influential to the regression results. The regression results will be altered if we exclude those cases.

![Residuals vs Leverage plot for detecting outliers or influential points](c:/users/pgalpern/onedrive - university of calgary/teaching/data603/data/datasets_pictures/residualvsleverage.png)





Case 1 is the typical look when there is no influential case, or cases. You can barely see Cook's distance lines (a red dashed line) because all cases are well inside of the Cook's distance lines. In Case 2, a case is far beyond the Cook's distance lines (the other residuals appear clustered on the left because the second plot is scaled to show larger area than the first plot). The plot identified the influential observation as #49. 


```{r }
Advertising=read.table("Advertising.txt", header = TRUE,  sep ="\t" )
morepower<-lm(sale~tv+I(tv^2)+I(tv^3)+I(tv^4)+I(tv^5)+I(tv^6)+I(tv^7)+I(tv^8)+
                I(tv^9)+I(tv^10)+I(tv^11)+radio+tv*radio, data=Advertising)
plot(morepower,which=5)
cubic<-lm(sale~tv+I(tv^2)+I(tv^3)+radio+tv*radio, data=Advertising)
plot(cubic,which=5)

```
From the Adertising example (both cubic and morepower models), you can see that data point 131 is an influential case. 

<!--From the Adertising example, you can see that there is no influential case. However, data point 131 and 6 are potentially an outlier as they differ from the majority of the cases in the data set.-->

__2. Cook's Distance__

A measure of the overall influence an outlying observation has on the estimated coefficients was proposed by R. D. Cook (1979). The **Cook's distance** $D_i$ measures the effect of deleting a given observation, and is interpreted for the $i$th observation as follows: 

A large value of $D_i$ indicates that the observed $Y_i$ value has strong influence on the estimated coefficients (since the residual, the leverage, or both will be large). A general rule of thumb is that observations with a Cook's $D$ of more than 3 times the mean, $\mu$, is a possible outlier. An alternative interpretation is to investigate any point over $4/n$, where $n$ is the number of observations. Other authors suggest that any "large" $D_i$ should be investigated. How large is "too large"? The consensus seems to be that a $D_i$ value of more that 1 indicates an influential value, but you may want to look at values above 0.5. Like the other numerical measures of influence, options for calculating Cook's distance are available in most statistical software packages. 


```{r}
Advertising=read.table("Advertising.txt", header = TRUE,  sep ="\t" )
morepower<-lm(sale~tv+I(tv^2)+I(tv^3)+I(tv^4)+I(tv^5)+I(tv^6)+I(tv^7)+I(tv^8)+I(tv^9)+I(tv^10)+I(tv^11)+radio+tv*radio, data=Advertising)
Advertising[cooks.distance(morepower)>0.5,] #have Cook statistics larger than 0.5
plot(morepower,pch=18,col="red",which=c(4))
```

__3. Leverage points__

Points that fall horizontally far from the line are points of high leverage; these points can strongly influence the slope of the least squares line. If one of these high leverage points does appear to actually invoke its influence on the slope of the line then we call it an influential point. Usually we can say a point is influential if, had we plotted the line without it, the influential point would have been unusually far from the least squares line.

Leverage values for multiple regression models are extremely difficult to calculate without the aid of a computer. Fortunately, most of the statistical software packages have options that give the leverage associated with each observation.

A good rule of thumb to identify an observation $y_i$ as influential if its leverage value $h_i$ is 

$$
\begin{aligned}
h_i>\frac{2p}{n} or\frac{3p}{n}
\end{aligned}
$$

where
$h_i$ is the leverage for the $i$th observation 

$p$ = the number of predictors

$n$= the number of the sample size


```{r,warning=F}
morepower<-lm(sale~tv+I(tv^2)+I(tv^3)+I(tv^4)+I(tv^5)+I(tv^6)+I(tv^7)+I(tv^8)+I(tv^9)+I(tv^10)+I(tv^11)+radio+tv*radio, data=Advertising)
lev=hatvalues(morepower)
p = length(coef(morepower))
n = nrow(Advertising)
outlier2p = lev[lev>(2*p/n)]
outlier3p = lev[lev>(3*p/n)]
print("h_I>2p/n, outliers are")
print(outlier2p)
print("h_I>3p/n, outliers are")
print(outlier3p)
plot(rownames(Advertising),lev, main = "Leverage in Advertising Dataset", xlab="observation",
    ylab = "Leverage Value")
abline(h = 2 *p/n, lty = 1)
abline(h = 3 *p/n, lty = 1)
```

Big question now is, once we identify an outlier, or influential observation, __what do we do with it?__

For a good understanding of the regression model, if we have some outliers or influential points, we may want to....

-See what happens when we exclude these from the model as an outlier has occured due to an error in data collection or recoding, then the solution is to simply remove the observation.

-Investigate these cases separately as it may happen that we mistyped.



## Inclass Practice Problem 21

From the clerical staff work hours, using residual plots to conduct a residual analysis of the data.  Check any potential outliers.  

```{r, include=F}
workhours=read.csv("CLERICAL.csv",
                header = TRUE) 
improvemodel<-lm(Y~X2+I(X2^2)+X4+X5,data=workhours)

#Residuals vs Leverage plot
par(mfrow=c(2,2))
plot(improvemodel)

# Leverage Point
lev=hatvalues(improvemodel)
p = length(coef(improvemodel))
n = nrow(workhours)
outlier = lev[lev>(2*p/n)]
print(outlier)
plot(rownames(workhours),lev, main = "Leverage in Advertising Dataset", xlab="observation",
    ylab = "Leverage Value")
abline(h = 2 *p/n, lty = 1)
abline(h = 3 *p/n, lty = 1)

#Cook Distance
Advertising[cooks.distance(improvemodel)>0.5,] #have Cook statistics larger than 0.1
plot(improvemodel,pch=18,col="red",which=c(4))
```

## Inclass Practice Problem 22

Check assumptions for the model below to to predict executive salary (Y)

$$
Y=\beta_0+\beta_1X_1+\beta_2X_1^2+\beta_3X_2+\beta_4X_3+\beta_5X_4+\beta_6X_5+\beta_7X_3*X_4+\epsilon 
$$

```{r , include=F}
library(mctest) #for VIF
library(lmtest)
salary=read.csv("EXECSAL2.csv", header = TRUE )
bestmodel<-lm(log(Y)~X1+I(X1^2)+X2+factor(X3)+X4+X5+factor(X3)*X4,data=salary)
summary(bestmodel)

# Linearity  and Normal assumptions
par(mfrow=c(2,2))
plot(bestmodel)

#Heteroscedasticity
bptest(bestmodel)
library(olsrr)
ols_test_f(bestmodel)

#Testing for Normality
shapiro.test(residuals(bestmodel))

# Multicollinearity
library(car)
firstordermodel<-lm(Y~X2+X4+X5,data=workhours)
vif(firstordermodel)

#Cook Distance
salary[cooks.distance(bestmodel)>0.5,] #have Cook statistics larger than 0.1
plot(bestmodel,pch=18,col="red",which=c(4))

# Leverage Point
lev=hatvalues(bestmodel)
p = length(coef(bestmodel))
n = nrow(salary)
outlier = lev[lev>(2*p/n)]
print(outlier)
plot(rownames(salary),lev, main = "Leverage in Advertising Dataset", xlab="observation",
    ylab = "Leverage Value")
abline(h = 2 *p/n, lty = 1)
abline(h = 3 *p/n, lty = 1)
```

## How to deal with Heteroscedasticity?

1. Log-transformation

As the simple solver, log-transformation can be one of the candidates. When log() takes the numbers, the difference between big and small numbers relatively becomes small. 

2. Box-Cox transformations. 

This will help the 'transformed' data to have equal variance, and, as usually happens, will also make the transformed data to follow a normal distribution. 

3.  Weight Least Squares Regression 

If one wants to correct for heteroskedasticity by using a fully efficient estimator rather than accepting inefficient OLS and correcting the standard errors, the appropriate estimator is weight least squares, which is an application of the more general concept of generalized least squares. 



## Box-Cox Transformations (Transformations for Nonnormallity and Heteroscedasticity)


Unequal variances and nonnormality of the error terms frequently appear together. To remedy these departures from a linear regression model, __we need a transformation on Y__, since the shapes and spreads of the distributions of Y need to be changed. Such a transformation on Y may also at the same time help to linearize a curvilinear regression relation. At other times, a simultaneous transformation on X may be needed to obtain or maintain a linear regression relation. 

__The Box Cox transformation__ is named after statistician George Box and Sir Daivd Roxbee Cox who collaborated on a 1964 paper and developed the technique.

It is often difficult to determine which transformation on response variable (Y) to use,the Box-Cox method consider a family of power transformations on __strictly positive response variables (all data is positive and greater than 0)__ such that $Y^{(\lambda)}=Y^{\lambda}$, where $\lambda$ is a parameter to be determined using the data, vaires from -5 to 5. Note that this family encompasses the following simple transformations:

$$
\begin{aligned}
\mbox{Common Box-Cox Transformations:}&\\
{\lambda}&=2 , Y^{'}=Y^2&\\
{\lambda}&=0.5 , Y^{'}=\sqrt{Y}&\\
{\lambda}&=0 , Y^{'}=log_eY \mbox{(by definition)}&\\
{\lambda}&=-0.5 , Y^{'}=1/\sqrt{Y}&\\
{\lambda}&=-1.0 , Y^{'}=1/Y&\\
\mbox{Note! the transformation for 0 is} \log_e(Y)&,\mbox{otherwise all data would transform 
to} Y^0=1
\end{aligned}
$$

A regression model with the response variable a member of the family of power transformations becomes: 

$$
\begin{aligned}
Y^{(\lambda)}_i&=\beta_0+\beta_1X_1+\beta_2X_2+...+\epsilon_i\\
where&\\
Y^{(\lambda)}_i &=
\begin{cases} 
\frac{Y^{\lambda}-1}{\lambda}, \lambda \neq0\\
log_e{Y,}\lambda=0
\end{cases}
\end{aligned}
$$

Note that the regression model includes an additional parameter,$\lambda$, which needs to be estimated. The Box-Cox procedure uses __the method of maximum likelihood__ to estimate$\lambda$ , as well as the other parameters$\beta_0,\beta_1,\beta_2,...$ In this way, the Box-Cox procedure identifies $\hat{\lambda}$  , the maximum likelihood estimate of A to use in the power transformation. 


__Example 1__ A practical question for the head of a university could be how study fees (stfees) raise the
universities net assets (nassets).A simple linear regression could help to explain the relation between these two variables.


```{r}
library(Ecdat)#for dataset University
reg=lm(nassets~stfees,data=University)
summary(reg)
#Testing for Homoscedasticity
library(lmtest)
bptest(reg)

#Testing for Normality
shapiro.test(residuals(reg))
```

```{r}

library(MASS) #for the boxcox()function
bc=boxcox(reg,lambda=seq(-1,1))
#extract best lambda
bestlambda=bc$x[which(bc$y==max(bc$y))]
bestlambda
```

From the output we found that __bestlambda is approximately between -0.2 to 0.2.__ Below are the outputs when we chose $\lambda$=0 and $\lambda$=0.1111.

```{r}
library(Ecdat)#for dataset University
bcmodel1=lm(log(nassets)~stfees,data=University)
summary(bcmodel1)

#Testing for Homoscedasticity
library(lmtest)
bptest(bcmodel1)

#Testing for Normality
shapiro.test(residuals(bcmodel1))

bcmodel2=lm((((nassets^0.1111)-1)/0.1111)~stfees,data=University)
summary(bcmodel2)

#Testing for Homoscedasticity
library(lmtest)
bptest(bcmodel2)

#Testing for Normality
shapiro.test(residuals(bcmodel2))

```


_Example 2_

Now we will use __the gala dataset__ as an example of using the Box-Cox method to justify a transformation for Multiple Linear Regression. There are 30 Galapagos islands and 7 variables in the dataset. The relationship between the number of plant species and several geographic variables is of interest. We fit an additive multiple regression model with Species as the response and most of the other variables as predictors.

_The dataset gala_ contains the following variables:

Species:the number of plant species found on the island

Endemics:the number of endemic species

Area:the area of the island ($km^2$)

Elevation:the highest elevation of the island (m)

Nearest:the distance from the nearest island (km)

Scruz:the distance from Santa Cruz island (km)

Adjacent:the area of the adjacent island (square km)

*Source:M. P. Johnson and P. H. Raven (1973) "Species number and endemism: The Galapagos Archipelago revisited" Science, 179, 893-895*

```{r}
library(faraway)#for the dataset gala
reg1=lm(Species ~ Area + Elevation + Nearest + Scruz + Adjacent, data = gala)
summary(reg1)
reg2=lm(Species ~ Elevation + Adjacent, data = gala)
summary(reg2)

#Testing for Homoscedasticity
library(lmtest)
bptest(reg2)
plot(reg2,which=1)

#Testing for Normality
shapiro.test(residuals(reg2))

```
```{r}
library(MASS) #for the boxcox()function
reg2=lm(Species ~ Elevation + Adjacent, data = gala)
summary(reg2)
bc=boxcox(reg2,lambda=seq(-1,1))

#extract best lambda
bestlambda=bc$x[which(bc$y==max(bc$y))]
bestlambda

```


Using the Box-Cox method, we see that $\lambda=0.2525$ falls both in the confidence interval, and is extremely close to the maximum, which suggests a transformation of the form

$$
\begin{aligned}
Y^{(0.2525)}_i &=\frac{Y^{0.2525}-1}{0.2525}.\\
Y^{(0.2525)}_i&=\beta_0+\beta_1Elevation+\beta_2Adjecent+\epsilon_i
\end{aligned}
$$
 
Will be used.We then fit a model with this transformation applied to the response.

```{r}
#We found that bestlambda=0.2525
bcmodel=lm((((Species^0.2525)-1)/0.2525) ~ Elevation + Adjacent, data = gala)
summary(bcmodel)

#Testing for Homoscedasticity
library(lmtest)
bptest(bcmodel)
plot(bcmodel,which=1)

#Testing for Normality
shapiro.test(residuals(bcmodel))
```

From the output, the resulting fitted versus residuals plot looks much better! Moreover, the output displays the Breusch-Pagan test that result from the Box-Cox model.  The p-value = 0.1836>0.05, indicating that we do not reject the null hypothesis. Therefore, the test  provide evidence to suggest that homoscedasticity does exist.Additinally, Shapiro-Wilk normality test also confirms that the residuals are normally distributed as the p-value=0.1724 >0.05. So Box-Cox Tranformation is helpful for these cases.



## Important points about OLS  Regression

1. There must be linear relationship between independent and dependent variables.

2. Multiple regression suffers from multicollinearity and heteroskedasticity.

3. Linear Regression is very sensitive to Outliers. It can terribly affect the regression line and eventually the forecasted values.

4. Multicollinearity can increase the variance of the coefficient estimates and make the estimates very sensitive to minor changes in the model. The result is that the coefficient estimates are unstable.


```{r}
credit=read.csv("credit.csv", header = TRUE)

nomultimodel2<-lm(Balance~Income+Limit+Age+Cards+factor(Student),data=credit)
summary(nomultimodel2)
intmodel<-lm(Balance~(Income+Limit+Age+Cards+factor(Student))^2,data=credit)
summary(intmodel)
bestintmodel<-lm(Balance~Income+Limit+Age+Cards+factor(Student)+Income*Limit+Income*factor(Student)+Limit*factor(Student),data=credit)
summary(bestintmodel)


library(ggplot2)
library(GGally)
pairs(~Balance+Income+Limit+Age+Cards+factor(Student),data=credit,panel=panel.smooth)

bestintmodel1<-lm(Balance~Income+I(Income^2)+Limit+I(Limit^2)+I(Limit^3)+I(Limit^4)+I(Limit^5)+I(Limit^6)+I(Limit^7)+I(Limit^8)+Age+Cards+factor(Student)+Income*Limit+Income*factor(Student)+Limit*factor(Student),data=credit)
summary(bestintmodel1)

#check linearity
#residual vs fitted data plot for the  model
ggplot(bestintmodel1, aes(x=.fitted, y=.resid)) +
  geom_point() +geom_smooth()+
  geom_hline(yintercept = 0) 



```

Improving model by using Log(X), but not recommended
```{r}
library(ggplot2)
Advertising=read.table("Advertising.txt", header = TRUE,sep ="\t" )
modellog<-lm(sale~log(tv)+radio+tv*radio, data=Advertising)
summary(modellog)
#residual vs fitted data plot for the simple  model
ggplot(modellog, aes(x=.fitted, y=.resid)) +
  geom_point() +geom_smooth()+
  geom_hline(yintercept = 0) 
```

