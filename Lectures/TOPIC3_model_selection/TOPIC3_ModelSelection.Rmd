---
title: "TOPIC3: Model selection"
output:
  pdf_document: default
  html_notebook: default
  html_document:
    df_print: paged
  word_document: default
  editor_options: 
    markdown: 
  markdown: 
    wrap: 72
---

    <style type="text/css">

    body{ /* Normal  */
          font-size: 18px;
      }
    td {  /* Table  */
      font-size: 8px;
    }
    .title {
      font-size: 38px;
      color: DarkRed;
    }
     p {line-height: 2em;}
    h1 { /* Header 1 */
      font-size: 28px;
      color: DarkBlue;
    }
    h2 { /* Header 2 */
        font-size: 22px;
      color: DarkBlue;
    }
    h3 { /* Header 3 */
      font-size: 18px;
      font-family: "Times New Roman", Times, serif;
      color: DarkBlue;
    }
    code.r{ /* Code block */
        font-size: 12px;
    }
    pre { /* Code block - determines code spacing between lines */
        font-size: 14px;
    }
    </style>

```{r message=FALSE, warning=FALSE, include=FALSE}
knitr::opts_knit$set(root.dir = 'c:/users/pgalpern.UC/onedrive - university of calgary/teaching/data603/data/datasets_pictures')
knitr::opts_chunk$set(fig.width=3, fig.height=3) 
```

## Multiple Linear Regression

## Part III: Model Selection

Â© Thuntida Ngamkham 2022 modified by Paul Galpern

## Model Selection

One of the biggest problem in building a model to describe a response variable ($Y$) is choosing the important independent variables to be included. The list of potentially important independent variables is extremely long and we need some objective methods of screening out those which are not important. The problem of deciding which of a large set of independent variables to include in a model is a common one.

**For example: Independent Variables in the Executive Salary**

Independent Variable and Description

x~1~: Experience (years)-quantitative

x~2~: Education (years)-quantitative

x~3~: Bonus eligibility (1 if yes, 0 if no)-qualitative

x~4~: Number of employees supervised-quantitative

x~5~: Corporate assets (millions of dollars)-quantitative

x~6~: Board member (1 if yes, 0 if no)-qualitative

x~7~: Age (years)-quantitative

x~8~: Company profits (past 12 months, millions of dollars)-quantitative

x~9~: Has international responsibility (1 if yes, 0 if no)-qualitative

x~10~: Company's total sales (past 12 months, millions of dollars)-quantitative

## Steps in Selecting the Best Regression Equation

To select the best regresson equation, carry out the following steps

1.  Specify the maximum model to be considered.

2.  Specify a strategy for selecting a model

3.  Evaluate the reliability of the model chosen.

By following theses steps, you can convert the fuzzy idea of finding the best predictors of $Y$ into simple, concrete action. Each step helps to ensure reliability and to reduce the work required.

## Step 1: Specifying the Maximum Model

The maximum model is defined to be the largest model (the one having the most predictor variables) considered at any point in the process of model selection. A model created by deleting predictors from the maximum model is called *a restriction of the maximum model*.

## Step 2: Specify a strategy for selecting a model

A systematic approach to building a restriction model from a large number of independent variables is difficult because the interpretation of multivariable interactions is complicated. We therefore turn to a screening procedure, available in most statistical software packages, objectively determine which independent variables in the list are the most important predictors of $Y$ and which are the least important predictors. The most widely used method is **stepwise regression**, while another popular method, **backward** and **forward regression**, also are provided in this section.

## Stepwise Regression Procedure

The user first identifies the response $y$ and the set of potentially important independent variables $x_1$, $x_2$,...,$x_p$, where $p$ is generally large. However, we often **include only the main effects** of both quantitative variables (first-order terms) and qualitative variables (dummy variables). The response and independent variables are then entered into the computer software, and the stepwise procedure begins.

**Step 1** The software program fits all possible one-variable models of the form $$
\begin{aligned}
E(Y)=\beta_0+\beta_1X_i
\end{aligned}
$$ to the data, where $X_i$ is the $i$th independent variable, $i = 1,2,...,p$. For each model, the t-test for a single $\beta_1$ parameter is conducted to test the null hypothesis

H~0~: $\beta_1$ = 0

against the alternative hypothesis

H~a~: $\beta_1\neq0$

The independent variable that produces the largest (absolute) t -value is then declared the best one-variable predictor of $Y$. Call this independent variable $X_1$.

```{r,warning=FALSE}
library(olsrr)#need to install the package olsrr
salary=read.csv("EXECSAL2.csv", header = TRUE)
model1<-lm(Y~X1, data = salary)
summary(model1)
model2<-lm(Y~X2, data = salary)
summary(model2)
model3<-lm(Y~X3, data = salary)
summary(model3)
model4<-lm(Y~X4, data = salary)
summary(model4)
model5<-lm(Y~X5, data = salary)
summary(model5)
model6<-lm(Y~X6, data = salary)
summary(model6)
model7<-lm(Y~X7, data = salary)
summary(model7)
model8<-lm(Y~X8, data = salary)
summary(model8)
model9<-lm(Y~X9, data = salary)
summary(model9)
model10<-lm(Y~X10, data = salary)
summary(model10)

```

**Step 2** The stepwise program now begins to search through the remaining $(p - 1)$ independent variables for the best two-variable model of the form

$\hat{Y}=\beta_0+\beta_1X_1+\beta_2X_i$

This is done by fitting all two-variable models containing $X_1$ and each of the other $(p - 1)$ options for the second variable $X_i$. The t-values for the test $H_0 : \beta_2 = 0$ are computed for each of the $p-1$ models (corresponding to the remaining independent variables, $X_i, i = 2, 3, . . . , p-1$), and the variable having the largest $t$ is retained. Call this variable $X_2$.

Before proceeding to Step 3, the stepwise routine will go back and check the t-value of $\hat{\beta_1}$ after $\hat{\beta_2}X_2$ has been added to the model. If the t-value has become nonsignificant at some specified $\alpha$ level (say $\alpha=0.3$), the variable $X_1$ is removed and a search is made for the independent variable with a $\beta$ parameter that will yield the most significant t-value in the presence of $\hat{\beta_2}X_2$.

The reason the t-value for $X_1$ may change from step 1 to step 2 is that the meaning of the coefficient $\hat{\beta_1}$ changes. In step 2, we are approximating a complex response surface in two variables with a plane. The best-fitting plane may yield a different value for $\hat{\beta_1}$ than that obtained in step 1. Thus, both the value of $\hat{\beta_1}$ and its significance usually changes from step 1 to step 2. For this reason, stepwise procedures that recheck the t-values at each step are preferred.

```{r,warning=FALSE}
library(olsrr)#need to install the package olsrr
salary=read.csv("EXECSAL2.csv", header = TRUE)
model1<-lm(Y~X1+X2, data = salary)
summary(model1)
model2<-lm(Y~X1+X3, data = salary)
summary(model2)
model3<-lm(Y~X1+X4, data = salary)
summary(model3)
model4<-lm(Y~X1+X5, data = salary)
summary(model4)
model5<-lm(Y~X1+X6, data = salary)
summary(model5)
model6<-lm(Y~X1+X7, data = salary)
summary(model6)
model7<-lm(Y~X1+X8, data = salary)
summary(model7)
model8<-lm(Y~X1+X9, data = salary)
summary(model8)
model9<-lm(Y~X1+X10, data = salary)
summary(model9)
```

**Step 3** The stepwise regression procedure now checks for a third independent variable to include in the model with $X_1$ and $X_2$. That is, we seek the best model of the form

$E(Y) = \beta_0 + \beta_1X_1 + \beta_2X_2 + \beta_3X_i$

To do this, the computer fits all the $(p-2)$ models using $X_1,X_2,$ and each of the $(p-2)$ remaining variables, $X_i$ , as a possible $X_3$. The criterion is again to include the independent variable with the largest (significant) t-value. Call this best third variable $X_3$. The better programs now recheck the t-values corresponding to the $X_1$ and $X_2$ coefficients, replacing the variables that yield nonsignificant t-values.

This procedure is continued until no further independent variables can be found that yield significant t-values (at the specified $\alpha$ level) in the presence of the variables already in the model.

Refer to the Executive Salary Example. A preliminary step in the construction of this model is the determination of the most important independent variables. For one firm, 10 potential independent variables (seven quantitative and three qualitative) were measured in a sample of 100 executives. The data are saved in the **EXECSAL2.CSV** file. Since it would be very difficult to construct a complete first-order model with all of the 10 independent variables, use stepwise regression to decide which of the 10 variables should be included in the building of the final model.

```{r,warning=FALSE}
library(olsrr)#need to install the package olsrr
salary=read.csv("EXECSAL2.csv", header = TRUE)
fullmodel<-lm(Y~X1+X2+factor(X3)+X4+X5+factor(X6)+X7+X8+factor(X9)+X10, data = salary)
summary(fullmodel)
stepmod=ols_step_both_p(fullmodel,pent = 0.1, prem = 0.3, details=TRUE)
summary(stepmod$model)
```

*R functions* *ols_step_both_p(): Build regression model from a set of candidate predictor variables by entering and removing predictors based on p values*

*Note!*

*pent: variables with p value less than pent will enter into the model.*

*prem: variables with p value more than prem will be removed from the model.*

*details: print the regression result at each step.*

From the output, the regression model is $Y=X_1+X_2+X_3+X_4+X_5+\epsilon$. Is this model the best fit for predicting executive salary?

## Inclass Practice Problem 10

From the credit example in MLR Modelling Part 2, use **Stepwise Regression Procedure** to find the potentially important independent variables for predicting credit card balance.

```{r,include=F}
library(olsrr) #need to install the package olsrr
credit=read.csv("credit.csv", header = TRUE)
fullmodel<-lm(Balance~Income+Limit+Rating+Cards+Age+Education+factor(Gender)+factor(Ethnicity)+factor(Married)+factor(Student),data=credit)
summary(fullmodel)
stepmod=ols_step_both_p(fullmodel,pent = 0.1, prem = 0.3, details=TRUE)
summary(stepmod$model)
```

```{r,include=F}
credit=read.csv("credit.csv",header = TRUE)

bestmodel2<-lm(Balance~Income+Limit+Rating+Cards+Age+factor(Student)+Income*Rating+Income*factor(Student)+Limit*Rating+Limit*factor(Student),data=credit)
summary(bestmodel2) 
```

```{r,include=F}
library(olsrr) #need to install the package olsrr
credit=read.csv("credit.csv", header = TRUE)
interactmodel<-lm(Balance~(Income+Limit+Rating+Cards+Age+factor(Student))^2,data=credit) ## This includes all combinations of two-way interactions
intermod=ols_step_both_p(interactmodel,pent = 0.1, prem = 0.3, details=TRUE)
summary(intermod$model)
```

```{r,include=F}
library(olsrr) #need to install the package olsrr
credit=read.csv("credit.csv", header = TRUE)
bestmodel1<-lm(Balance~Income+Limit+Rating+Cards+Age+factor(Student)+Rating*Limit+Rating*Income+factor(Student)*Income+factor(Student)+Limit+Rating*Age+Income*Age,data=credit) ## This includes all 
summary(bestmodel1)
```

## Backward Elimination Procedure

The Backward procedure initially fits a model containing terms for all potential independent variables. That is, for $p$ independent variables, the model $E(Y) = \beta_0 + \beta_1X_1 + \beta_2X_2 +...+\beta_pX_p$ is fit in step 1. The variable with the smallest t (or F) statistic for testing $H_0 :\beta_i = 0$ is identified and dropped from the model if the t-value is less than some specified critical value or p-value more than a cut-off. The model with the remaining $(p-1)$ independent variables is fit in step 2, and again, the variable associated with the smallest nonsignificant t-value is dropped. This process is repeated until no further nonsignificant independent variables can be found.

```{r}
library(olsrr) #need to install the package olsrr
salary=read.csv("EXECSAL2.csv", header = TRUE)
fullmodel<-lm(Y~X1+X2+factor(X3)+X4+X5+factor(X6)+X7+X8+factor(X9)+X10, data = salary)
backmodel=ols_step_backward_p(fullmodel, prem = 0.3, details=TRUE)
summary(backmodel$model)
```

*R functions* *ols_step_backward_p():Build regression model from a set of candidate predictor variables by removing predictors based on p values*

From the output, the first order regression model by using Backward Regression Procedure is $Y=\beta_0+\beta_1X_1+\beta_2X_2+\beta_2X_3+\beta_3X_4+\beta_4X_5+\beta_5X_9+\epsilon$. Consider the predictor X9 has tcal=-1.287 with the p-value= 0.201, this predictor should be dropped out from the output. Therefore, $Y=\beta_0+\beta_1X_1+\beta_2X_2+\beta_2X_3+\beta_3X_4+\beta_4X_5+\epsilon$ is the first order model to predict salary by using For Backward Regression Procedure.

## Inclass Practice Problem 11

From the credit example in MLR Modelling Part 2, use **Backward Regression Procedure** to find the potentially important independent variables for predicting credit card balance.

```{r,include=F}
library(olsrr) #need to install the package olsrr
credit=read.csv("credit.csv", header = TRUE)
fullmodel<-lm(Balance~Income+Limit+Rating+Cards+Age+Education+factor(Gender)+factor(Ethnicity)+factor(Married)+factor(Student),data=credit)
backmodel=ols_step_backward_p(fullmodel,prem = 0.3,details=TRUE)
summary(backmodel$model)
```

## Forward selection procedure

This method is nearly identical to the stepwise procedure previously outlined. The only difference is that the forward selection technique provides no option for rechecking the t-values corresponding to the $X$'s that have entered the model in an earlier step.

```{r}
library(olsrr) #need to install the package olsrr
salary=read.csv("EXECSAL2.csv", header = TRUE)
fullmodel<-lm(Y~X1+X2+factor(X3)+X4+X5+factor(X6)+X7+X8+factor(X9)+X10, data = salary)
formodel=ols_step_forward_p(fullmodel,penter = 0.1, details=TRUE)
summary(formodel$model)
```

*R functions* *ols_step_forward_p():Build regression model from a set of candidate predictor variables by entering predictors based on p values* *penter: p value; variables with p value less than penter will enter into the model. By default, penter=0.3*

From the output, we specified our penter = 0.1 to follow the same procedure of Stepwise regression. Therefore, the regression model by using Forward Regression Procedure is

$$
Y=\beta_0+\beta_1X_1+\beta_2X_2+\beta_3X_3+\beta_4X_4+\beta_5X_5+\epsilon. 
$$

## Inclass Practice Problem 12

From the credit example in MLR Modelling Part 2, use **Forward Regression Procedure** to find the potentially important independent variables for predicting credit card balance.

```{r,include=F}
library(olsrr) #need to install the package olsrr
fullmodel<-lm(Balance~Income+Limit+Rating+Cards+Age+Education+factor(Gender)+factor(Ethnicity)+factor(Married)+factor(Student),data=credit)
formodel=ols_step_forward_p(fullmodel,penter = 0.1,details=TRUE)
summary(formodel$model)
```

**Note!**

R also provides a function for selecting a subset of predictors from a larger set. You can use stepwise selection (backward,forward,both) by using the stepAIC() function from the MASS package. This function will select variable by extracting AIC (AIC value is explained in the next topic).

**CAUTION** Be aware of using the results of stepwise regression to make inferences about the relationship between $E(Y)$ and the independent variables in the first order model.

**First**, an extremely large number of t-tests have been conducted, leading to a high probability of making more Type I errors.

**Second**, stepwise regression should be used only when necessary- that is when you want to determine which of a large number of potentially important independent variables should be used in the model building process.

## All-Possible-Regressions Selection Procedure

We presented stepwise regression as an objective screening procedure. Stepwise does not only provide the largest t-value, but also the techniques differ with respect to the criteria for selecting the ''best'' subset of variables. In this section, we describe four criteria widely used in practice,

**1.** $R^2$ Criterion the multiple coefficient of determination

$$
\begin{aligned}
R^2=\frac{SSR}{SST}=1-\frac{SSE}{SST}
\end{aligned}
$$

will increase when independent variables are added to the model. Therefore, the model that includes all $p$ independent variables $E(Y) =\beta_0 +\beta_1X_1 + \beta_2X_2 +...+\beta_pX_p$ will yield the largest $R^2$.

<!--In practice, the best model found by the $R^2$ criterion will rarely be the model with the largest $R^2$. Generally, you are looking for a simple model that is as good as, or nearly as good as, the model with all $p$ independent variables. But unlike that in stepwise regression, the decision about when to stop adding variables to the model is a subjective one. -->

**2. Adjusted** $R^2$ or RMSE Criterion

We can use the adjusted $R^2$ instead of $R^2$. It is easy to show that $R^2_{adj}$ is related to MSE as follows: $$
\begin{aligned}
R^2_{adj}&=1-\frac{\frac{SSE}{n-p-1}}{\frac{SST}{n-1}}\\
R^2_{adj}&=1-(n-1)\frac{MSE}{SST}\\
s&=RMSE=\sqrt{\frac{1}{n-p-1}SSE}
\end{aligned}
$$ Note that $R^2_{adj}$ increases only if RMSE decreases [since SST remains constant for all models]. Thus, an equivalent procedure is to search for the model with the minimum, or near minimum, RMSE.

**3. Mallows's Cp Criterion**

The Cp criterion, named for Colin Lingwood Mallow, selects as the best subset model with

(1) a small value of Cp (i.e., a small total mean square error), means that the model is relatively precise.

(2) a value of Cp near p + 1, a property that indicates that slight (or no) bias exists in the subset regression model.

Thus, the Cp criterion focuses on minimizing total mean square error and the regression bias. If we are mainly concerned with minimizing total mean square error, we will want to choose the model with the smallest Cp value, as long as the bias is not large. On the other hand, we may prefer a model that yields a Cp value slightly larger than the minimum but that has slight (or no) bias.

**4. AIC (Akaike's information criterion)**

When using the model to predict $Y$, some information will be lost. Akaike's information criterion estimates the relative information lost by a given model. It is defined as

$$
\begin{aligned}
AIC=n\ln(\frac{SSE}{n})+2p
\end{aligned}
$$

The formula is formulated by the statistician **Hirotugu Akaike**. Models with smaller values of AIC are preferred.

*Where*

*n : the number of observations in the dataset*

*p : the number of parameters in the model*

**5. BIC (Bayesian information criteria)**

Bayesian information criterion (BIC) is another criterion for model selection. It is based, in part, on the likelihood function, and it is closely related to Akaike information criterion (AIC). The models can be tested using corresponding BIC values. Lower BIC value indicates a better model.^2^

$$
\begin{aligned}
BIC=n\ln(\frac{SSE}{n})+(p)\ln(n)
\end{aligned}
$$

*Note!*

*n : the number of observations in the dataset*

*p : the number of parameters in the model*

In this class, we are going to use R software package to calculate all values.

```{r,warning=FALSE}
# Option 1
library(olsrr)
salary=read.csv("EXECSAL2.csv", header = TRUE)
firstordermodel<-lm(Y~X1+X2+X3+X4+X5+X6+X7+X8+X9+X10, data= salary)
#Select the subset of predictors that do the best at meeting some well-defined objective criterion, such as having the largest R2 value or the smallest MSE, Mallow's Cp or AIC.
ks=ols_step_best_subset(firstordermodel, details=TRUE)
# for the output interpretation
rsquare<-c(ks$rsquare)
AdjustedR<-c(ks$adjr)
cp<-c(ks$cp)
AIC<-c(ks$aic)
cbind(rsquare,AdjustedR,cp,AIC)
par(mfrow=c(2,2)) # split the plotting panel into a 2 x 2 grid
plot(ks$cp,type = "o",pch=10, xlab="Number of Variables",ylab= "Cp")
plot(ks$rsq,type = "o",pch=10, xlab="Number of Variables",ylab= "R^2")
plot(ks$aic,type = "o",pch=10, xlab="Number of Variables",ylab= "AIC")
plot(ks$adjr,type = "o",pch=10, xlab="Number of Variables",ylab= "Adjusted R^2")
```

*R functions*

*ols_step_best_subset: Best subsets regression, select the subset of predictors that do the best at meeting some well-defined objective criterion, such as having the largest* $adj R^2$ value or the smallest MSE, Mallow's Cp or AIC.^1^ BIC values are not provided

```{r,warning=TRUE}
#  Option 2
library(olsrr)
salary=read.csv("EXECSAL2.csv", header = TRUE)
firstordermodel<-lm(Y~X1+X2+X3+X4+X5+X6+X7+X8+X9+X10, data= salary)

library(leaps) #need to install the package leaps for regsubsets() function
best.subset<-regsubsets(Y~X1+X2+X3+X4+X5+X6+X7+X8+X9+X10, data= salary, nv=10 ) 
#by default, regsubsets() only reports results up to the best 8-variable model
#Model selection by exhaustive search, forward or backward stepwise, or sequential replacement
#The summary() command outputs the best set of variables for each model size using RMSE.
summary(best.subset)
reg.summary<-summary(best.subset)

# for the output interpretation
rsquare<-c(reg.summary$rsq)
cp<-c(reg.summary$cp)
AdjustedR<-c(reg.summary$adjr2)
RMSE<-c(reg.summary$rss)
BIC<-c(reg.summary$bic)
cbind(rsquare,cp,BIC,RMSE,AdjustedR)
par(mfrow=c(3,2)) # split the plotting panel into a 3 x 2 grid
plot(reg.summary$cp,type = "o",pch=10, xlab="Number of Variables",ylab= "Cp")
plot(reg.summary$bic,type = "o",pch=10, xlab="Number of Variables",ylab= "BIC")
plot(reg.summary$rsq,type = "o",pch=10, xlab="Number of Variables",ylab= "R^2")
plot(reg.summary$rss,type = "o",pch=10, xlab="Number of Variables",ylab= "RMSE")
plot(reg.summary$adjr2,type = "o",pch=10, xlab="Number of Variables",ylab= "Adjusted R^2")
```

*R functions*

*regsubsets() :performs best sub- set selection by identifying the best model that contains a given number of predictors. No AIC values are provided*

------------------------------------------------------------------------

From the output, the first order regression model is $Y=\beta_0+\beta_1X_1+\beta_2X_2+\beta_3X_3+\beta_4X_4+\beta_5X_5+\epsilon$. Is this model the best fitted model for predicting executive salary?

## Inclass practice Problem 13

From the credit card example, using **All Possible Regressions Selection Procedure** to analyse which independent predictors should be used in the model.

```{r,include=F}
# First option
library(leaps) #need to install the package leaps for best.subset() function
credit=read.csv("credit.csv",header = TRUE)
#the best fit model from unit 1 part 2
fullmodel<-lm(Balance~Income+Limit+Rating+Cards+Age+Education+factor(Gender)+factor(Ethnicity)+factor(Married)+factor(Student),data=credit)
ks=ols_step_best_subset(fullmodel, details=TRUE)
par(mfrow=c(2,2)) # split the plotting panel into a 2 x 2 grid
plot(ks$cp,type = "o",pch=10, xlab="Number of Variables",ylab= "Cp")
plot(ks$rsq,type = "o",pch=10, xlab="Number of Variables",ylab= "R^2")
#plot(ks$rss, xlab="Number of Variables",ylab= "RMSE")
plot(ks$aic,type = "o",pch=10, xlab="Number of Variables",ylab= "AIC")
plot(ks$adjr,type = "o",pch=10, xlab="Number of Variables",ylab= "Adjusted R^2")

# We use here the second option but we can use the first option
#by default, regsubsets() only report results up to the best 8-variable model
best.subset<-regsubsets(Balance~Income+Limit+Rating+Cards+Age+Education+factor(Gender)+factor(Ethnicity)+factor(Married)+factor(Student), data= credit, nv=10 ) #by default, regsubsets() only report results up to the best 8-variable model

summary(best.subset)
reg.summary<-summary(best.subset)

# for the output interpretation
rsquare<-c(reg.summary$rsq)
cp<-c(reg.summary$cp)
AdjustedR<-c(reg.summary$adjr2)
RMSE<-c(reg.summary$rss)
cbind(rsquare,cp,RMSE,AdjustedR)
par(mfrow=c(2,2)) # split the plotting panel into a 2 x 2 grid
plot(reg.summary$cp,type = "o",pch=10, xlab="Number of Variables",ylab= "CP")
plot(reg.summary$rsq,type = "o",pch=10, xlab="Number of Variables",ylab= "R^2")
plot(reg.summary$rss,type = "o",pch=10, xlab="Number of Variables",ylab= "RMSE")
plot(reg.summary$adjr2,type = "o",pch=10, xlab="Number of Variables",ylab= "Adjusted R^2")
```

```{r,include=F}
credit=read.csv("credit.csv",header = TRUE)
#the best fit model from unit 1 part 2

model7<-lm(Balance~Income+Limit+Rating+Cards+Age+factor(Gender)+factor(Student),data=credit)
summary(model7)

```

## 3. Evaluate the reliability of the model chosen.

After using model selection by automatic methods or all possible regression methods, we might not have the best fit model yet, as we consider only main effects on independent variables. After eliminating some variables that are not important out of the model, we consider interaction terms and/or high order multiple regression model to improve the model.

```{r}
salary=read.csv("EXECSAL2.csv", header = TRUE )

firstordermodel<-lm(Y~X1+X2+factor(X3)+X4+X5,data=salary)
summary(firstordermodel)

# building the best model with interation term
interacmodel<-lm(Y~(X1+X2+factor(X3)+X4+X5)^2,data = salary)
summary(interacmodel)

bestinteracmodel<-lm(Y~X1+X2+factor(X3)+X4+X5+factor(X3)*X4,data=salary) 
summary(bestinteracmodel)

#considering high order model between Xs and Y to improve the model
library(GGally) # need to install the GGally package for ggpairs function
#option 1: using function ggpairs()
salarydata <-data.frame(salary$Y,salary$X1,salary$X2,salary$X3,salary$X4,salary$X5)
salarydata
#ggpairs(salarydata)
#LOESS or LOWESS: LOcally WEighted Scatter-plot Smoother
ggpairs(salarydata,lower = list(continuous = "smooth_loess", combo =
 "facethist", discrete = "facetbar", na = "na"))
#option2: using function pairs()
pairs(~Y+X1+X2+factor(X3)+X4+X5,data=salary,panel = panel.smooth) 

bestmodel<-lm(Y~X1+I(X1^2)+X2+factor(X3)+X4+X5+factor(X3)*X4,data=salary)
summary(bestmodel)
bestmodel1<-lm(Y~X1+I(X1^2)+I(X1^3)+X2+factor(X3)+X4+X5+factor(X3)*X4,data=salary)
summary(bestmodel1)
```

*R Functions* *ggpairs(): look at all pairwise combinations of continuous variables in scatterplots.* *pairs(): optional function for pairwise combinations* *panel.smooth: add a smooth loess curve on the scatters*

From the output, you can see that after including an interaction term ($X_3*X_4$) and quadratic term $X^2_1$, they led to such a big improvement in the model as following,

1.  all the p-values \< 0.05, which means that all regression coefficients were significantly non-zero.

2.  $R^2_{adj}$ increases from 0.9164 to 0.9355

3.  Standard error of residuals (RMSE) decreases from 0.07512 to 0.06596

Therefore, it is clear that adding the additional terms really has led to a better fit to the data.

## Inclass practice Problem 14

From the credit card example, when we investigate the scatter plots for all pairwise combinations between variables, find the best fitted model to predict balance. You may include interaction terms and higher order terms to improve the model.

```{r,include=TRUE}



credit=read.csv("credit.csv",
                header = TRUE) 
Interacmodel=lm(Balance~(Income+Rating+Age+Limit+Cards+factor(Student))^2,data=credit)
summary(Interacmodel)

bestinteractmodel<-lm(Balance~Income+Rating+Age+Limit+Cards+factor(Student)+Income*Rating+Income*Age+Income*factor(Student)+Rating*Limit+Rating*factor(Student)+Limit*factor(Student),data=credit)
summary(bestinteractmodel)

bestinteractmodel1<-lm(Balance~Income+Rating+Age+Limit+Cards+factor(Student)+Income*Rating+Income*factor(Student)+Rating*Limit+Limit*factor(Student),data=credit)
summary(bestinteractmodel1)

anova(bestinteractmodel1,bestinteractmodel) ## This shows that we can remove the two interactions together


library(GGally) # need toinstall the GGally package for ggpairs function

creditdata <-data.frame(credit$Balance,credit$Income,credit$Rating,credit$Age,credit$Limit,credit$Cards,credit$Student)
ggpairs(creditdata,lower = list(continuous = "smooth_loess", combo =
  "facethist", discrete = "facetbar", na = "na"))

pairs(~Balance+Income+Limit+Rating+Cards+Age+factor(Student),data=credit,panel = panel.smooth)

bestinteractmodel2<-lm(Balance~Income+I(Income^2)+Rating+I(Rating^2)+Age+Limit+Cards+factor(Student)+Income*Rating+Income*factor(Student)+Rating*Limit+Limit*factor(Student),data=credit)
summary(bestinteractmodel2)


```

## Inclass Practice Problem 15

Clerical staff work hours. In any production process in which one or more workers are engaged in a variety of tasks, the total time spent in production varies as a function of the size of the work pool and the level of output of the various activities.

For example, in a large metropolitan department store, the number of hours worked (Y) per day by the clerical staff may depend on the following

variables:

X1 = Number of pieces of mail processed (open, sort, etc.)

X2 = Number of money orders and gift certificates sold,

X3 = Number of window payments (customer charge accounts) transacted ,

X4 = Number of change order transactions processed ,

X5 = Number of checks cashed ,

X6 =Number of pieces of miscellaneous mail processed on an ''as available'' basis , and

X7 =Number of bus tickets sold

The data are provided in **CLERICAL.csv** file count for these activities on each of 52 working days. Conduct a Stepwise Regression Procedure and All-Possible-Regressions procedure of the data using R software package.

```{r,include=F}
#Individual T -Test
workhours=read.csv("CLERICAL.csv",header = TRUE) 
firstordermodel<-lm(Y~X1+X2+X3+X4+X5+X6+X7,data=workhours)
summary(firstordermodel)
model<-lm(Y~X2+X4+X5,data=workhours)
summary(model)
anova(model,firstordermodel)

#Model1
mod1=lm(Y~X2+X4+X5 , data=workhours)
interactmodel1<-lm(Y~(X2+X4+X5)^2, data=workhours)
summary(interactmodel1)

bestmodel1<-lm(Y~X2+X4+X5, data=workhours)
summary(bestmodel1)
```

```{r,include=F}
#Improving model Individual T test
library(olsrr) #need to install the package olsrr
library(GGally) # need toinstall the GGally package for ggpairs function
library(leaps) #need to install the package leaps for best.subset() function

#ggpairs(workhours)
#pairs(~Y+X1+X2+X3+X4+X5+X6+X7,data=workhours)

bestmodel11<-lm(Y~X2+I(X2^2)+X4+X5, data=workhours)
summary(bestmodel11)
bestmodel12<-lm(Y~X2+I(X2^2)+X4+X5+I(X5^2), data=workhours)
summary(bestmodel12)

```

```{r,include=F}
#_Stepwise Method_
library(olsrr) #need to install the package olsrr
library(leaps) #need to install the package leaps for best.subset() function

workhours=read.csv("CLERICAL.csv",header = TRUE) 

#Using Stepwise Regression Procedure for data selection
firstordermodel<-lm(Y~X1+X2+X3+X4+X5+X6+X7,data=workhours)
step=ols_step_both_p(firstordermodel,pent = 0.1, prem = 0.3, details=TRUE)
summary(step$model)

```

```{r,include=F}
library(olsrr) #need to install the package olsrr
library(leaps) #need to install the package leaps for best.subset() function
workhours=read.csv("CLERICAL.csv",header = TRUE) 

#Model2
mod2=lm(Y~X2+X4+X5+X6, data=workhours)
interactmodel2<-lm(Y~(X2+X4+X5+X6)^2, data=workhours)
summary(interactmodel2)

bestmodel21<-lm(Y~X2+X4+X5+X6+X2*X6, data=workhours)
summary(bestmodel21)

bestmodel22<-lm(Y~X2+X4+X5+X6, data=workhours)
summary(bestmodel22)


```

```{r,include=F}
#Improving model from Stepwise method
library(olsrr) #need to install the package olsrr
library(GGally) # need toinstall the GGally package for ggpairs function

workhours=read.csv("CLERICAL.csv",
                header = TRUE) 

#ggpairs(workhours)
#pairs(~Y+X1+X2+X3+X4+X5+X6+X7,data=workhours)

bestmodel21<-lm(Y~X2+I(X2^2)+X4+X5+X6+I(X6^2), data=workhours)
summary(bestmodel21)

bestmodel22<-lm(Y~X2+I(X2^2)+X4+X5+X6, data=workhours)
summary(bestmodel22)
```

```{r,include=F}
#Using All possible Regression
library(olsrr) #need to install the package olsrr
library(leaps) #need to install the package leaps for best.subset() function


workhours=read.csv("CLERICAL.csv",
                header = TRUE) 

#option 1
ks=ols_step_best_subset(firstordermodel, details=TRUE)
par(mfrow=c(2,2)) # split the plotting panel into a 2 x 2 grid
plot(ks$cp,type = "o",pch=10, xlab="Number of Variables",ylab= "Cp")
plot(ks$rsq,type = "o",pch=10, xlab="Number of Variables",ylab= "R^2")
#plot(ks$rss, xlab="Number of Variables",ylab= "RMSE")
plot(ks$aic,type = "o",pch=10, xlab="Number of Variables",ylab= "AIC")
plot(ks$adjr,type = "o",pch=10, xlab="Number of Variables",ylab= "Adjusted R^2")


#option 2
best.subset<-regsubsets(Y~X1+X2+X3+X4+X5+X6+X7, data= workhours)
summary(best.subset) 
reg.summary<-summary(best.subset)
rsquare<-c(reg.summary$rsq)
cp<-c(reg.summary$cp)
AdjustedR<-c(reg.summary$adjr2)
RMSE<-c(reg.summary$rss)
cbind(rsquare,cp,RMSE,AdjustedR)
par(mfrow=c(2,2)) # split the plotting panel into a 2 x 2 grid
plot(reg.summary$cp,type = "o",pch=10, xlab="Number of Variables",ylab= "CP")
plot(reg.summary$rsq,type = "o",pch=10, xlab="Number of Variables",ylab= "R^2")
plot(reg.summary$rss,type = "o",pch=10, xlab="Number of Variables",ylab= "RMSE")
plot(reg.summary$adjr2,type = "o",pch=10, xlab="Number of Variables",ylab= "Adjusted R^2")
```

```{r,include=F}
workhours=read.csv("CLERICAL.csv",header = TRUE) 
mod3=lm(Y~X1+X2+X3+X4+X5+X6, data=workhours)
interactmodel3<-lm(Y~(X1+X2+X3+X4+X5+X6)^2, data=workhours)
summary(interactmodel3)
bestmodel3<-lm(Y~X1+X2+X3+X4+X5+X6+X2*X6+X1*X6, data=workhours)
summary(bestmodel3)
```

```{r,include=F}
#Improving model from best subset function
library(GGally) # need toinstall the GGally package for ggpairs function

workhours=read.csv("CLERICAL.csv",header = TRUE) 

#ggpairs(workhours)
#pairs(~Y+X1+X2+X3+X4+X5+X6+X7,data=workhours)


bestmodel31<-lm(Y~X1+X2+I(X2^2)+X3+X4+X5+X6+X1*X6, data=workhours)
summary(bestmodel31)

```

^1^<https://www.rdocumentation.org/packages/olsrr/versions/0.5.3/topics/ols_step_best_subset>

^2^[https://medium.com/\@analyttica/what-is-bayesian-information-criterion-bic-b3396a894be6](https://medium.com/@analyttica/what-is-bayesian-information-criterion-bic-b3396a894be6){.uri}

## References

*-Gareth James & Daniela Witten & Trevor Hastie Robert Tibshirani, An Introduction to Statistical Learning with Applications in R: Springer New York Heidelberg Dordrecht London.*

*-Wickham and Grolemund, R for Data Science: O'Reilly Media*
